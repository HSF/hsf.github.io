<!DOCTYPE html>
<html>
<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1 user-scalable=no">
    <title>HSF Generator Workshop, 26-28 November 2018</title>
    <link rel="icon" type="image/x-icon" href="/images/hsf_logo_angled.png">

    <!-- bootstrap framework -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.5/flatly/bootstrap.min.css">

    <link rel="stylesheet" href="/css/hsf.css" type="text/css" />
</head>
<body>

<div class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
          <a href="/" class="navbar-brand"><span class="glyphicon glyphicon-home"></span>  HSF</a>
          <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
        </div>
        <div class="navbar-collapse collapse" id="navbar-main">
          <ul class="nav navbar-nav navbar-center">
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="/index.html" id="wg_menu"><span class="glyphicon glyphicon-briefcase"></span>  Working Groups<span class="caret"></span></a>
                <ul class="dropdown-menu" aria-labelledby="activities_menu">
                  <li><a href="/what_are_WGs.html">What are HSF working groups?</a></li>
                  <li class="divider">
                  
                    <li><a href="/workinggroups/dataanalysis.html">Data Analysis</a></li>
                  
                    <li><a href="/workinggroups/detsim.html">Detector Simulation</a></li>
                  
                    <li><a href="/workinggroups/frameworks.html">Frameworks</a></li>
                  
                    <li><a href="/workinggroups/generators.html">Physics Generators</a></li>
                  
                    <li><a href="/workinggroups/juliahep.html">JuliaHEP - Julia in HEP</a></li>
                  
                    <li><a href="/workinggroups/pyhep.html">PyHEP - Python in HEP</a></li>
                  
                    <li><a href="/workinggroups/recotrigger.html">Reconstruction and Software Triggers</a></li>
                  
                    <li><a href="/workinggroups/toolsandpackaging.html">Software Developer Tools and Packaging</a></li>
                  
                    <li><a href="/workinggroups/training.html">HSF Training</a></li>
                  
               </ul>
            </li>
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="/index.html" id="activities_menu"><span class="glyphicon glyphicon-briefcase"></span>  Activities<span class="caret"></span></a>
                <ul class="dropdown-menu" aria-labelledby="activities_menu">
                 <li><a href="/what_are_activities.html">What are HSF activity areas?</a></li>
                 <li class="divider">
                 
                   <li><a href="/activities/analysisfacilitiesforum.html">Analysis Facilities Forum</a></li>
                 
                   <li><a href="/activities/conditionsdb.html">Conditions Databases</a></li>
                 
                   <li><a href="/activities/differentiablecomputing.html">Differentiable Computing</a></li>
                 
                   <li><a href="/activities/gsdocs.html">Season of Docs</a></li>
                 
                   <li><a href="/activities/gsoc.html">Google Summer of Code</a></li>
                 
                   <li><a href="/activities/idds.html">intelligent Data Delivery Service</a></li>
                 
                   <li><a href="/activities/licensing.html">Licensing</a></li>
                 
                   <li><a href="/activities/reviews.html">Reviews</a></li>
                 
                   <li><a href="/activities/visualization.html">Visualisation</a></li>
                 
               </ul>
            </li>
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="/index.html" id="meetings_menu"><span class="glyphicon glyphicon-phone-alt"></span>  Meetings<span class="caret"></span></a>
                <ul class="dropdown-menu" aria-labelledby="activities_menu">
                 
                   <li><a href="/meetings/compute-accelerator-forum.html">Compute Accelerator Forum</a></li>
                 
                   <li><a href="/meetings/coordination.html">Coordination Meetings</a></li>
                 
                   <li><a href="/meetings/roundtable.html">Software and Computing Roundtable</a></li>
                 
                   <li><a href="/meetings/software-forum.html">Software Forum</a></li>
                 
               </ul>
            </li>
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="/index.html" id="communication_menu"><span class="glyphicon glyphicon-bullhorn"></span>  Communication<span class="caret"></span></a>
                <ul class="dropdown-menu" aria-labelledby="communication_menu">
                  <li><a href="/future-events.html">Community Calendar</a></li>
                  <li><a href="https://indico.cern.ch/category/5816/" target="_hsf_indico">Meetings: Indico</a></li>
                  <li><a href="/Schools/events.html">Training Schools</a></li>
                  <li class="divider">
                  <li><a href="/forums.html">Mailing Lists and Forums</a></li>
                  <li><a href="/organization/minutes.html">Meeting Notes</a></li>
                  <li><a href="/technical_notes.html">Technical Notes</a></li>
                  <li><a href="/organization/documents.html">Documents</a></li>
                  <li><a href="/material_for_presentations.html">Material for presentations</a></li>
                  <li class="divider">
                  <li><a href="/newsletter.html">Newsletters</a></li>
                  <li><a href="/events.html">Events &amp; Workshops</a></li>
                  <li><a href="https://www.youtube.com/c/HEPSoftwareFoundation" target="_hsf_youtube">HSF on YouTube</a></li>
                  <li class="divider">

                  <li><a href="/inventory/inventory.html">HSF Project Inventory</a></li>
                </ul>
            </li>
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="/projects.html" id="projects_menu"><span class="glyphicon glyphicon-road"></span>  Projects &amp; Support<span class="caret"></span></a>
                <ul class="dropdown-menu" aria-labelledby="projects_menu">
                  <li><a href="/project_guidelines.html">How to join as a project</a></li>
                  <li><a href="/projects.html">Member Projects</a></li>
                  <li class="divider">
                  <li><a href="/services.html">Development Services</a></li>
                </ul>
            </li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="/index.html" id="about_menu"><span class="glyphicon glyphicon-info-sign"></span>  About<span class="caret"></span></a>
              <ul class="dropdown-menu" aria-labelledby="about_menu">
                <li><a href="/get_involved.html">Get involved!</a></li>
<li>
                </li>
<li><a href="/organization/team.html">The Coordination Team</a></li>
                <li><a href="/organization/planning/plan-of-work-2023.html">HSF Planning</a></li>
                <li class="divider">
                <li><a href="/organization/goals.html">Goals of the HSF</a></li>
                <li><a href="/organization/cwp.html">Community White Paper</a></li>
                <li><a href="/organization/hsf-letters.html">Letters from the HSF</a></li>
                <li><a href="/organization/presentations.html">HSF Presentations</a></li>
                <li><a href="/organization/youtube.html">HSF YouTube Channel</a></li>
                <li class="divider">
                <li><a href="/howto-website.html">Website Howto</a></li>
              </ul>
            </li>
          </ul>
        </div>
    </div>
</div>


<div class="container">

<div class="PageNavigation">

  <!-- Note that page sorting is reverse time ordered, so "next" refers to the previous
       meeting in time and "previous" to the next one -->
  
   <p class="alignleft"><a href="/organization/2018/11/21/softwareforum.html"> ← HSF Software Forum on EIC Software Consortium, November 21, 2018</a></p>
  

  
    <p class="alignright"><a href="/organization/2018/11/29/coordination.html">HSF Weekly Meeting #152, 29 November, 2018 → </a></p>
  

</div>

<div style="clear: both;"></div>


<hr>

  <h1 id="hsf-generator-workshop-26-28-november-2018">HSF Generator Workshop, 26-28 November 2018</h1>

<p>This live notes document is available to any participant in the workshop
to write down important points or conclusions from the workshop and to
identify areas where we will want to follow up or need to resolve open
questions.</p>

<p>Please use it - it’s an essential tool for the workshop’s memory!</p>

<p>The Organisers</p>

<h2 id="monday-am">Monday AM</h2>

<h4 id="cwp-challenges-and-workshop-aims-liz-sexton-kennedy">CWP Challenges and Workshop Aims (Liz Sexton-Kennedy)</h4>
<ul>
  <li>Generators critical part of what experiments do. Aware that
“reward system” has limitations, need to improve on this. HSF
CWP suggests reviewing software credits. HSF CWP has a section on
generators and one on training and careers.</li>
  <li>Estimated CPU computing resources needed for HL-LHC exceed those
that are affordable on a “flat budget”. Pileup dramatically
increases complexity. Need major reengineering of the software in
all areas to be able to fully exploit HL-LHC potential. Especially
need to exploit parallelism and vectorization of modern
architectures, and GPUs are even more challenging.</li>
  <li>Physics of interest at HL-LHC is at smaller cross sections and
higher multiplicity.</li>
  <li>Generators CAN be computationally intensive. Larger consumption in
ATLAS than CMS. ATLAS US wall clock budget in 2016 had around 20%
spent on generators.</li>
  <li>NLO and beyond NLO pose large challenges. Theorist community
continues to push development a higher orders. HL-LHC will drive
need to <del>generate events</del> make predictions accurate at higher
order.</li>
  <li>Recap from Run1 to Run4.
    <ul>
      <li>Run1 mainly LO, O(100ms).</li>
      <li>Run2 moved to NLO with O(10-100s) execution time.</li>
      <li>Total grid times in an experiment depends on mixture of LO and
NLO required by analysts.</li>
      <li>ATLAS/CMS create O(10B) events per year, rule of thumb is MC =
1-2x data events.</li>
      <li>Run4 will collect 10x events as in Run2. Will need more
precision hence higher orders. Will have more particles, with
more phase space.</li>
      <li>All within a flat budget…</li>
    </ul>
  </li>
  <li>Modernization means: many core, SIMD vectorization, NUMA memory
hierarchy, offload to GPU/FPGA/TPU accelerators.
    <ul>
      <li>Need to inject engineering help. Move towards Open Source
development is a way to achieve this.</li>
    </ul>
  </li>
  <li>Agenda of the workshop explained:
    <ul>
      <li>Introduction to generators for this mixed audience</li>
      <li>Experiment needs</li>
      <li>Technical requirements on modern codes</li>
      <li>Status of main generators. <em><span class="underline">Can we
agree on a way to benchmark them?</span></em>
</li>
      <li>Ongoing work on HPCs</li>
      <li>Event reweighting? Share samples between ATLAS and CMS?</li>
      <li>Status of generators used as “decayers”</li>
      <li>NNLO status</li>
      <li>Follow-up planning</li>
      <li>Hackathon</li>
    </ul>
  </li>
  <li>C/ZachMarshall: reference list in CMS is a twiki, but also in
ATLAS. There is also a common twiki between the experiments and
the theorists can contribute</li>
  <li>C/Zach about HL-LHC: collect 10x events, do we need 10x MC events?
It depends on balance between statistical and systematic errors.
In many cases we are limited by statistics and we do not need
that.</li>
  <li>C/StefanRoiser: for LHCb please get in touch with Patrick
Koppenburg for reference lists</li>
  <li>C/StefanoFrixione: not sure how many people read references, e.g.
name of madgraph generator</li>
  <li>C/StefanoFrixione: not sure what the needed precision at HL-LHC
will be. A/Liz: hope that we get an answer to this question from
the workshop.</li>
  <li>C/AndyBuckley: generator codes are already “open” — but these are
not things that can just be dipped into and have significant
improvements made, as many have discovered. Impossible to do
without close collaboration &amp; discussion with authors… who will
approve a huge &amp; invasive pull request that hasn’t been previously
discussed? Code hosting systems have also moved toward modern
web-driven interfaces, but I don’t think that’s been a real
limiting factor.</li>
</ul>

<h4 id="introduction-to-generator-codes-simon-platzer">Introduction to Generator Codes (Simon Platzer)</h4>
<ul>
  <li>Introduction to generators for a mixed audience.</li>
  <li>Factorization at different energy scales: hard process
calculation, parton shower algorithms, multiple interaction
models, hadronization models. Event weight comes from hard cross
section.
    <ul>
      <li>Hard process is exact calculation in QCD/QED perturbation
theory.</li>
      <li><em>Most developments and computing power requirements sit in
hard process calculation and parton shower algorithms.</em></li>
    </ul>
  </li>
  <li>Hard process calculation and parton showers.
    <ul>
      <li>Jacobian factor to map unit hypercube of phase space to
physical momenta.</li>
      <li>At high momentum transfer, expand QCD in LO, NLO, NNLO and so
on. <em><span class="underline">Real and virtual contributions
are separately divergent and cancel out. This introduces
subtraction terms which lead to negative weights, this is
unavoidable.</span></em>
</li>
      <li>Landscape of infrared sensitive observables. At every fixed
order of expansion, need more and more logarithm terms.</li>
      <li>Factorization of emissions. Resummation and parton showers.
NLO matching.</li>
      <li>This is all state of the art for all major generators. In
order to produce infrared safe predictions, negative weights
are unavoidable.</li>
    </ul>
  </li>
  <li>Event generator structure is based on the factorization discussed
at the beginning</li>
  <li>Challenges
    <ul>
      <li>Negative weights spoil probabilistic interpretation</li>
    </ul>
  </li>
  <li>Q/AndreaValassi: has the issue of negative weights hit us already
or not and how much? A/Liz and Simone Amoroso: it has hit us
already, but this is only where NLO is needed. For instance some
SUSY analysis went back from NLO to LO because of this. Simone:
note that <em><span class="underline">where NLO is needed and there
are 40% of events with negative weights, we need 25x more events
to generate! If you can stay at LO, you have no negative
weights.</span></em>
    <ul>
      <li>C/Zbiszek: note that at LEP times the problem of negative
weights was already there for QED but was eventually solved.</li>
      <li>C/StefanoFrixione: disagree, IR is only one part of the
problem leading to negative weights, and it is very difficult
to design a catch-all solution.</li>
    </ul>
  </li>
</ul>

<h4 id="atlas-needs-and-concerns-josh-mcfayden">ATLAS Needs and Concerns (Josh McFayden)</h4>
<ul>
  <li>Generation is 20% in projected 2028 needs for ATLAS. But the slice
will become more important as improvements come in fast detector
simulation and elsewhere.</li>
  <li>Majority of CPU comes from sherpa v2.2 v+jets setup, by far the
largest and most precise samples.</li>
  <li>Average CPU/event: 80s evgen, 240s fullsim, 45s fastsim, 60ds
reco.</li>
  <li>Thanks for all recent improvements to the theory community!</li>
  <li>Problems!
    <ul>
      <li>Negative weights. If the fraction is &gt;25%, using full
detector simulation becomes impractical.</li>
      <li>Precision vs CPU. More precision needs more CPU.</li>
      <li>Efficiently populating extreme regions of phase space.</li>
    </ul>
  </li>
  <li>Possible routes out
    <ul>
      <li>Can experiments help in generator development? Investigating
funding of positions by the experiments.</li>
      <li>Sacrifice speed for modelling/precision? Choose faster
generator? Reweight lower precision to higher precision?</li>
    </ul>
  </li>
  <li>PRELIMINARY benchmarking
    <ul>
      <li>W+jets (2jets NLO and 4jets LO). Sherpa vs MG5_aMC+Py8.
Larger CPU and memory consumption for sherpa, marginally
larger fraction of negative weights for MG/aMC.</li>
      <li>Note that fraction of negative weights is not a relevant
metric. You also need to know the spread of the values of the
negative weights to understand their impact.
<em><span class="underline">Need to design a better metric for
negative weights.</span></em>
</li>
      <li>ttbar comparison, also including Herwig and powheg</li>
    </ul>
  </li>
  <li>Infrastructure
    <ul>
      <li>Common computing infrastructure for ATLAS/CMS?</li>
      <li>Will generators run on CPUs or GPUs in 2028?</li>
      <li>ATLAS and CMS sharing samples?</li>
    </ul>
  </li>
  <li>Summary
    <ul>
      <li>Many improvements in recent years</li>
      <li>Certainly low hanging fruits. Event generation has not been
highly scrutinised until now.</li>
      <li>Optimisation can help, has to discuss how</li>
    </ul>
  </li>
  <li>Q: on slide 4, how much of generation is on the grid? A: do use
the external sherpa pre-integration</li>
</ul>

<h4 id="cms-needs-and-concerns-efe-yazgan">CMS Needs and Concerns (Efe Yazgan)</h4>
<ul>
  <li>Most LHC events are accompanied by hard jets, which are required
or vetoed in many analyses, hence need MC predictions for them.</li>
  <li>Most commonly used in CMS: MG5_aMC@NLO+Py8</li>
  <li>CMS software is C++ driven by python. MC production submitted to
grid resources, software available via cvmfs. External generators
called from CMSSW through an externalLHEProducer module.
    <ul>
      <li>Gridpacks: tarballs with precompiled code and initial phase
space integration results. They are placed in cvmfs and
accessed by remote jobs. In production, significant time spent
in untarring the tarballs.</li>
    </ul>
  </li>
  <li>In Run2, generators were 1-10% of total CPU (per MC event,
depending on type of MC event - not a % relative to full grid
budget). Multi-leg NLO has negative weights up to 40%, larger
samples needed.</li>
  <li>Beyond Run2: much larger samples, larger alternative samples for
systematic studies, precise tails of distributions need more
precise predictions (NLO, hence negative weights)</li>
  <li>Table with use of event weights for systematic studies</li>
  <li>MG5_aMV bias weights to reweight LO to NLO (see O. Mattelaer’s
arxiv)</li>
  <li>Needed developments
    <ul>
      <li>Timing comparison and understanding in ATLAS and CMS</li>
      <li>Reduction of negative weights at NLO</li>
      <li>Multi threading, vectorization, GPUs, reduce memory
consumption</li>
      <li>Faster phase space integration (neural networks, GPUs:
10-5000x faster possible?)</li>
      <li>Collaboration ATLAS-CMS, support positions</li>
    </ul>
  </li>
  <li>C/Servesh: estimates 10-5000 faster depend on how you compute
this. C/Peter: in that paper, faster was for things that are
already fast!</li>
  <li>Q/slide 7: why the difference in gridpacks between MG and sherpa?
A: not sure</li>
  <li>Q/AndreaValassi slide 12: do you actually see problems fitting
jobs in 2GB per core for generation? A/Liz: in CMS we do share
geometry in multi threaded framework</li>
  <li>C/StefanoFrixione: not sure it’s a good idea to use the same
generator evel events between experiments. Good that experiments
cross check each other. A/Graeme: this will be discussed more in
detail in tomorrow’s discussion.</li>
</ul>

<h4 id="lhcb-needs-and-concerns-phil-ilten">LHCb Needs and Concerns (Phil Ilten)</h4>
<ul>
  <li>
<em><span class="underline">LHCb designed as B experiment. Decays are
the most delicate part and go through EvtGen.</span></em> Handle
~5000 signal decays. Then LHCb expanded physics program and uses
multiple purpose generators. 90% goes through multi-parton
interaction (NOT message passing interface MPI!).</li>
  <li>All MC production is done centrally. Every job selects a model and
an event type (i.e. a decay description).</li>
  <li>GAUSS internals description. Includes pileup typically in pythia8
and decay typically in EvtGen, plus ME and shower for specific
generator. Pileup tool generally included at fixed luminosity due
to luminosity levelling in LHCb running. Large number of
generators for general purpose, soft and hard processes.</li>
  <li>Recent developments to cope with Run3: filtered events, multiple
trigger conditions, redecay, particle gun events. Gaussino in
progress to be experiment-agnostic, for other experiments and also
theorists. Also looking at options for faster detector
simulations, including GAN and DELPHES.</li>
  <li>Q/Liz on thread safety: did you discuss this for EvtGen?
A/MichalKreps: will discuss this tomorrow</li>
  <li>Q/StefanoFrixione: are you using Pythia6? A: no, working on
eliminating it</li>
  <li>Q/AndreaValassi: no problem with negative weights? A: not now but
we will have a problem eventually</li>
  <li>Q/Servesh: how do you load C++ libraries? A: typically compile
time rather than dynamic. C/Servesh: good, compile time is much
faster.</li>
  <li>Q/SimoneAmoroso: so basically mostly run Pythia8 standalone? What
is your event generation budget? A/MichalKreps: for many cases it
is not a limiting factor as Geant4 is much more intense. But if
the user imposes cuts to save time on Geant4, this puts more
pressure on generation. A/GloriaCorti: yes in these cases
generation is the dominant budget. And
<em><span class="underline">event generation budget fraction will get
worse as we go to a faster detector simulation</span></em>. We don’t
run Pythia8 standalone but together with detector simulation.</li>
  <li>N.B. MPI in this talk means <em>multi-parton interactions</em>!</li>
</ul>

<h4 id="practical-computing-considerations-david-lange">Practical Computing Considerations (David Lange)</h4>
<ul>
  <li>MC production (generators plus detector simulation) is the major
consumer of CPU on the grid. Requests come in bursts, driven by
analysis deadlines like conferences. Gridpacks are a major
enterprise for non experts.</li>
  <li>Two approaches between experiments:
    <ul>
      <li>CMS like: gen+simulation, pileup+digi+trigger, reco. Good:
generator is small part of gen+sim. Bad: generators change
frequently, Geant4 you want stable instead across years.</li>
      <li>ATLAS like: also separate gen from simulation. Allows more
flexibility in generator software.</li>
    </ul>
  </li>
  <li>Grid runs 500k+ cores across 100 sites. Job slots 1-8 cores and
1-2 days maximum wall clock time. HPC resources are spiky.</li>
  <li>Aggressive code optimization of reco led to very large
improvements in time/event.</li>
  <li>Code should be robust to run B events. Startup time should be
minutes not hours. Code should be runnable as a library in the
experiment specific framework.</li>
  <li>Common wisdom “generators small compared to the rest”, this is
incorrect. Examples: gridpack untarring, move from LO to NLO and
NNLO, filters with low efficiency for selecting events.</li>
  <li>Examples of coding practices that enabled reco software
improvements.</li>
  <li>Examples of recent generator improvements in CMS. Had one NLO
generation workflow where 99%+ was the initialization of NLO
generator. Got a 3X improvement by improving specific functions
and by doing initialization exactly once.</li>
  <li>Software licenses
    <ul>
      <li>HEP converged on open source, but there are many such licenses</li>
      <li>Experiment software becomes GPL by inheritance if it uses GPL
components (although “derivative work” interpretation is a
very open discussion). And GPL2 and GPL3 are not even
compatible with each other.</li>
      <li>Coming up now because experiments are trying to solidify their
situations. CERN recently chose Apache2 to ease collaboration
with industry.</li>
      <li>We have to improve on software citation.</li>
      <li>C/StefanRoiser: for LHCb licensing was a long and complex
business. Chose GPL3 in the end.</li>
    </ul>
  </li>
  <li>C/StefanRoiser on workflows: LHCb is spending 70 to 80% of
resources only in Geant4, will keep generators together because
this makes it easier to run on unpledged resources without input
data.</li>
</ul>

<h4 id="discussion">Discussion</h4>
<ul>
  <li>Q/PhilIlten: how efficient are ATLAS/CMS in setting generator
level cuts for specific users? A/JoshMF: depends on the sample,
various approaches.
    <ul>
      <li>A/Zach: the closer to the ME you can put the cuts the faster
you manage to get. <em><span class="underline">Can some hooks be
added to generators in a common standard way to set generator
level cuts?</span></em>
</li>
    </ul>
  </li>
  <li>C/VitalianoCiulli: CMS some time ago was doing something like
redecays, it was more reshowering. We had to do something inside
Pythia to achieve that, but maybe it was not documented even if it
is in principle reusable. A/PhilIlten: working with PeterSkands to
do this in a more reusable way.</li>
  <li>C/StephenMrenna about inheritance.</li>
  <li>Q/JoshMF: <em><span class="underline">did not really manage to
understand yet why the ATLAS and CMS budget for generation is so
different</span></em>. A/Graeme: very difficult to answer this
question here, but we should identify at the workshop ways of
working on this, by putting the relevant people
together.</li>
</ul>

<h2 id="monday-pm">Monday PM</h2>

<h4 id="performance-optimisation-introduction---why-should-we-care-servesh-muraidham-it-di-wlcg-up-team-cern">Performance Optimisation Introduction - Why should we care? (Servesh Muraidham, IT-DI-WLCG, UP Team, CERN)</h4>
<ul>
  <li>Technical optimization is the focus, it can not be an after coding
activity</li>
  <li>Moore’s Law still working after 42 years but Denard scaling was
stopped by power consumption.</li>
  <li>GPU FP performance for non-terminated lines is ether flat or
dropping</li>
  <li>Post free lunch - Agricultural performance is improving danger in
lock in to a specific architecture, note the Xeon Phi cautionary
tale</li>
  <li>Roofline Performance Comparison - represents how well an algorithm
for a specific architecture - defines the concept of operational
intensity
    <ul>
      <li>DGEMM is the Linpack benchmark</li>
      <li>Recommend finding the flops/ byte for your algorithms</li>
      <li>Cautionary tale of the chinese supercomputer - a top entry at
SC but not useful for science - on the other hand some SC
sites take input on kernel benchmarks to use</li>
    </ul>
  </li>
  <li>Vector Operation discussion</li>
  <li>60% of the compiler implementation is devoted to loop optimization
so it is wise to allow the compiler to understand your loops</li>
  <li>Algorithms can be classified into 4 types, figure out which one
yours is suited to</li>
  <li>Ideal programs have computational kernels - HEP codes don’t have
them, why? Maybe too many years of optimizing hot spots?</li>
  <li>Discussion of the topic of the cost of virtual functions and
templates</li>
</ul>

<h4 id="optimising-memory-use-sebastien-ponce-cern">Optimising Memory Use (Sebastien Ponce, CERN)</h4>
<ul>
  <li>The memory problem - relative to CPU speeds memory access is
extremely slow (100x), mitigated by hierarchical cache structure</li>
  <li>Make sure stack variables are &lt;1kbyte, within current scope but
allocation is almost free</li>
  <li>Big objects go on the heap but they are very costly to allocate</li>
  <li>Pro-Con struct of array vs. array of pointers, the latter scatters
the memory and creates many small allocations - the latter will
thrash the cache</li>
  <li>Contiguous memory blocks must be used to limit the thrashing</li>
  <li>When using a vector of structs make sure you use “reserve”</li>
  <li>Array of Structures vs. Structure of Arrays which is best depends
on the algorithm applied to the data SofA would be bad for a sort.
SofA is best for vectorizable algorithms</li>
  <li>If the consumer of your data has different algorithms there is no
best answer (no magic)</li>
  <li>Profiler tools, 2 types:
    <ul>
      <li>Vtune from Intel - uses internal processor counters, gprof is
in the same category</li>
      <li>Callgrind - open source and simulation a processor, downs side
is this is slow</li>
    </ul>
  </li>
</ul>

<p>Don’t share data across cores, spread them across threads, another
reason to use stack variables</p>

<h4 id="performance-discussion">Performance Discussion</h4>
<ul>
  <li>Is deep inheritance expensive? - yes if it virtual , avoid
multiple inheritance , do use the initializer list with empty
constructor bodies, this removes initialization ordering fragility</li>
  <li>Branch predictions and look ahead should be automatic with good
code , why don’t we have good code? Servesh says we are
overwhelming the front end with lots of instructions. If we moved
the code to be SIMD efficient it would alleviate this problem
automatically.</li>
  <li>How do you strike a balance between reliability and optimization?
A: With the tools available today, there is no longer a conflict.
You can have fast and readable code.</li>
  <li>Don’t be afraid of specializing templates</li>
  <li>Don’t be afraid to rewrite your prototype code</li>
  <li>Understand the limitations of the abstraction and concept
programming you do</li>
  <li>Shouldn’t you code for the common case first not the corner one?
Yes but remember the memory wall and think about how your
algorithms use the data first.</li>
  <li>Do code reviews help in readability? Experts think yes.</li>
</ul>

<h4 id="sherpa-status-and-plans---marek-schonherr-cern">Sherpa Status and Plans - Marek Schonherr, CERN</h4>
<ul>
  <li>The framework:
    <ul>
      <li>Two ME generators…</li>
    </ul>
  </li>
  <li>Available physics simulations</li>
  <li>Changing scales and PDFs / alphaS(massZ)</li>
  <li>Code structure diagram - modular component structure loaded
dynamically - same technology used for user definable functions</li>
  <li>Run Strategy 1. Initialization, 2. Integration, 3. Event
generation
    <ul>
      <li>The first two steps can be done once and results are platform
independent</li>
      <li>Step three is done on the grid</li>
      <li>The second step is MPI parallizable</li>
    </ul>
  </li>
  <li>Code performance tables
    <ul>
      <li>Confident that x10 size performance improvements are possible
for the worst case NLO sample</li>
      <li>There are places where small compromises in physics
performance can lead to major technical performance
improvements</li>
    </ul>
  </li>
  <li>Walk through callgrind plots - at LO unweighting procedure
dominates time - at NLO also dominated by unweighting and matching</li>
  <li>Weight distributions and how they relate to performance - need a
better approximation of integrand</li>
  <li>If we have to fall back to LO because of performance it is better
to use the HT prime approximation</li>
  <li>Can there be a way to use static linking when in production and
don’t need the flexibility? A: it can be done but Taylor found
that he only gained 20% (doesn’t sound bad to Markus)</li>
</ul>

<h4 id="amc_nlo-status-and-plans---olivier-mattelaer">aMC_@NLO Status and Plans - Olivier Mattelaer</h4>
<ul>
  <li>The uncertainty <a href="https://github.com/NLO" class="user-mention">@NLO</a> - it is possible to trade speed for disk
space, allow reweighting with future pdf</li>
  <li>Limitations of the re-weighting</li>
  <li>Re-weighting for mass scan</li>
  <li>NLO re-weighting</li>
  <li>MadGraph is compiled with (-01) but not much better at higher
level (tested at O3)
    <ul>
      <li>-Ofast makes it 30% faster at LO/NLO -&gt; however it requires
validation since messes with the math functions</li>
    </ul>
  </li>
  <li>MPI strategies</li>
  <li>Timing results and resulting mpi scaling as a function of the
number of ranks - there is a point where you lose by adding ranks</li>
  <li>LO doesn’t scale to higher then 500-200 rank however it doesn’t
need high bandwidth communication so can run on a tier2</li>
  <li>HTC vs HPC chart - argues that HTC is better for constant cost -&gt;
started discussion of owned vs. borrowed resources</li>
  <li>GPUs - plots comparing CPU to GPU performance as a function of the
number of jets, also as a function of number of gluons in the
final state</li>
  <li>CPU vs. GPU - no clear winner but likely winner is GPU</li>
  <li>Q: Is the MG5 port to CUDA in 2011 still working? A: yes, could be
improved
    <ul>
      <li>C: very good, we need any code running on GPUs to exploit
latest HPCs</li>
    </ul>
  </li>
  <li>Q: Is MPI only for integration? Generation? Or both? A: it is only
for the integration</li>
</ul>

<h4 id="pythia-8-status-and-plans---philip-ilten">PYTHIA 8 Status and Plans - Philip Ilten</h4>
<ul>
  <li>History - lasted release will appear in March, another planned</li>
  <li>Documentation - lots of references</li>
  <li>Decided to be user-friendly</li>
  <li>Focuses on everything but ME generation</li>
  <li>Phythia is thread-safe except for adaptive maximum</li>
  <li>Special constructors for multiple instances that save time</li>
  <li>External pointers in the interface allowing lots of customization,
even hooks for Heavy Ion generation</li>
  <li>User defined hooks can interrupt and customize different parts of
the internal work flow of Pythia</li>
  <li>Supports both HepMC2 and HepMC3 and can read in LHEF, SLHA</li>
  <li>News -
    <ul>
      <li>HI support with collective effects and alternate models for
string fragmentation
        <ul>
          <li>Developed by Leif Lonnblad and Christian Bierlich JHEP
1810 (2018) 134</li>
          <li>Can use gridpacks</li>
        </ul>
      </li>
      <li>Full integration of DIRE and VINCIA parton showers</li>
      <li>Lots more see slide 11</li>
    </ul>
  </li>
  <li>DIRE from Stefan**2</li>
  <li>VINCIA by Peter Skands and Nadine Fischer</li>
  <li>Shower Variations by Stephen Mrenna and Peter Skands</li>
  <li>Dark Matter Models by Nishita Desai</li>
  <li>Deuteron Production by Phil Ilten</li>
  <li>Merging algorithms are slow and they are working to improve them
almost a bug fix</li>
  <li>Phythia CPU consumption may be small but it is used everywhere so
the integral can be significant - can you work on the MPI? A: Yes</li>
  <li>LHC specificity happened in the transition from P6 -&gt; P8</li>
</ul>

<h4 id="herwig-status-and-plans---peter-richardson-cern">Herwig Status and Plans - Peter Richardson, CERN</h4>
<ul>
  <li>Designed for hadronic physics</li>
  <li>Strong emphasis on accurate simulation of QCD</li>
  <li>Became unmaintainable so moved to C++ with Herwig7 - understanding
the code was of high value</li>
  <li>Tried to balance performance vs physics and maintainability</li>
  <li>They rely on externals much less because it turned out that the
interfaces to externals were more prone to error</li>
  <li>Structure chart</li>
  <li>Done with fortran conversion</li>
  <li>Improved NLO matching</li>
  <li>Work on uncertainties and reweighting</li>
  <li>Improvements in soft physics</li>
  <li>Recent work on Baryonic Colour Reconnection</li>
  <li>Future</li>
  <li>All of these are physics improvements - that’s what gets noticed
in the theory community and gets funding and jobs</li>
  <li>In Herwig the biggest speed/memory improvement would be to improve
NLO</li>
</ul>

<h4 id="powheg-status-and-plans---emanuele-re-cern--lapth">POWHEG Status and Plans - Emanuele Re, CERN &amp; LAPTh</h4>
<ul>
  <li>Status of the repository
    <ul>
      <li>A legacy version in svn</li>
      <li>V2 also in svn but more modular components</li>
      <li>RES with just a few processes</li>
    </ul>
  </li>
  <li>Powheg needs external parton shower</li>
  <li>Recent physics Applications
    <ul>
      <li>W+W- very computationally expensive for decays at NNLO+PS</li>
      <li>More examples on slide 3</li>
    </ul>
  </li>
  <li>Default output is unweighted events - at times weighted events are
needed</li>
  <li>Negative weights can appear despite the acronym in the name
    <ul>
      <li>Can be limited through “folding” -&gt; runs become longer as
number of calls to real matrix elements increases</li>
      <li>Folding costs time but for complicated processes it is a
balancing act</li>
    </ul>
  </li>
  <li>Different steps can be parallelized but there is a synchronization
point between steps</li>
  <li>Can detect “bad” (unstable points) grids to avoid including them
in the next step</li>
  <li>… more facilities on 5</li>
  <li>Some facilities come from interaction with experiments and this
will likely continue</li>
  <li>No effort to support new architectures or increased
parallelization</li>
  <li>NNLOPS reweighting works but it’s CPU intensive</li>
  <li>MiNLO merging for processes with light partons in the final state
    <ul>
      <li>there is a proposal that suggests a possible avenue for further
progress</li>
    </ul>
  </li>
</ul>

<h4 id="discussion-1">Discussion</h4>
<ul>
  <li>Josh: is benchmarking exercise redone for each release? (a general
audience questions) - it’s useful, we hope that support for this
will continue</li>
  <li>Can there be unit tests for physics validation?
    <ul>
      <li>Yes, this is done for Herwig made difficult by multiple
changes occurring per release which is benchmarked</li>
    </ul>
  </li>
  <li>Validation discussion
    <ul>
      <li>As complex for the generators to evaluate expected changes as
for the experiments</li>
      <li>High statistics are often needed - implying a large resource
pool</li>
    </ul>
  </li>
</ul>

<h2 id="tuesday-am">Tuesday AM</h2>

<h4 id="high-multiplicity-multi-jet-merging-with-hpc-technology">High-multiplicity multi-jet merging with HPC technology</h4>
<ul>
  <li>Traditional approaches to generating high-multiplicity jets does
not scale.</li>
  <li>HDF5 is used as it’s optimal for parallel access on HPCs.</li>
  <li>Processing compressed HDF5 files is x2 faster than the
uncompressed files.</li>
  <li>Each jet multiplicity is done explicitly and separately, but they
do need to be added together (merging with CKKW-L).</li>
  <li>Scaling is good, but not perfect - still being investigated.</li>
  <li>Q. HSF is working on “ferries” to transform between data types -
<strong>HDF5 seems very useful for LHE data, can this be made available
as a library?</strong>
    <ul>
      <li>Conversion of 65M event files from LHE XML to HDF5 took 17
hours (single thread). (Note: that’s 1kHz, ~1MB/s.)</li>
      <li>Added direct HDF5 output as a plugin for Sherpa, will be in
future releases.</li>
      <li>SciDAC projects have to produce “open source” products.</li>
    </ul>
  </li>
  <li>Q. What inter-rank communications is there? A. For particle level
more or less none. For the ME this is done using existing Sherpa
MPI implementation.</li>
  <li>Q. Is it time to update the LHE format?</li>
  <li>SP. Pythia has added direct support for reading these HDF5 files,
will be in a future release (once stabilized).</li>
  <li>FS. Can move the expensive parts (ME) to HPC, then just do the
particle level within the experiment software. Good also for
sharing ME between experiments and with the theory community. How
reusable are the MEs? A. Could tighten cuts, but would become a
bit less efficient. However, quite some variations in what can be
done at the particle level (blue), so expensive ME generation
(red) can be reused, saving a lot of
time.</li>
</ul>

<h4 id="a-novel-workflow-of-generator-tunings-in-hpc-for-lhc-new-physics-searches">A novel workflow of generator tunings in HPC for LHC new physics searches</h4>
<ul>
  <li>Q. HPC specific part is generating MC events, which is trivially
parallel? A. Yes, no parallel communication, so HTC would also be
fine for this.</li>
  <li>Q. Do you need load balancing between the ranks? A. This is
handled by the DIY toolkit (also used in previous example). Phil
noted that imbalances between per-event times are a real issue, so
this kind of dynamic balancing is very useful.</li>
  <li>Advantage of load balancing on a cluster is that far less
bookkeeping is needed in the end (cf. really independent jobs +
merging on the grid).</li>
  <li>Q. Did you consider Bayesian optimisation method, which offers
advantages of priors, etc.? A. Yes, it’s been considered, but not
yet really explored.</li>
</ul>

<h4 id="adaptive-multi-channel-integration-with-mpi">Adaptive multi-channel integration with MPI</h4>
<ul>
  <li>Implementing MPI is not just a technical task - one needs to
understand the program’s purpose, so needs physics input.</li>
  <li>Use MPI features - they work very well and are written and
supported by experts (don’t reinvent this wheel, you will fail!)</li>
  <li>Shared cluster made benchmarking difficult.</li>
  <li>Hybrid parallelisation provides an extra speed-up (OpenMP + MPI).</li>
  <li>Q. Does the integration do adaptive scheduling? A. No. Would be a
future improvement.</li>
  <li>Q. How difficult is it to collect the grids? Minor grouping or
just one big reduction? A. It’s just done on the master, so that
is a slow part (serial) at the end.</li>
  <li>OpenMP/MPI is also useful for NUMA architectures.</li>
</ul>

<h4 id="optimising-generator-usage">Optimising Generator Usage</h4>
<ul>
  <li>1 - Reduce CPU requirements by running large scale event
generation at LO
    <ul>
      <li>FS. Q. If things are flat in phase space it’s easy, but if not
flat then each analysis needs to apply their own corrections
(per <em>analysis</em>). So wouldn’t you need N(N)LO at the detector
level anyway? So what is the gain?
        <ul>
          <li>Can save a factor x10 in CPU. (FS was sceptical)</li>
          <li>No negative weights at LO.</li>
          <li>Generation itself is also faster.</li>
        </ul>
      </li>
      <li>NLO merged you can do at the truth level.</li>
      <li>Stefano. Q Generating 8 jets is technically impressive, but is
it needed - problems in the merging? A. Yes, the merging needs
to logarithmically be as exact as the ME.
        <ul>
          <li>FS - there was some work that showed this is under
control.</li>
          <li>Can use Holger’s work to do cross checks.</li>
        </ul>
      </li>
      <li>StefanH - NNLO is coming, but will not be as efficient as LO
simulations.
        <ul>
          <li>Stefano - not saying anything about NNLO. NNLO will be
doing a small number of processes only. LO can do
everything.</li>
          <li>NLO matched calculations do exist for inclusive processes.
MC limited, not ME limited.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>2 - Reduce computing costs by sharing unweighted matrix-element
event samples between experiments
    <ul>
      <li>Few input parameters.</li>
      <li>Q. Josh - what are technical hurdles for sharing files between
CMS and ATLAS? A. Liz, did not work in the past. Now can share
this in the same way that we do for OpenData, so technical
limitations not there anymore.
        <ul>
          <li>Just the computational part - not the
showering/hadronisation, so storage requirements are
minimal.</li>
        </ul>
      </li>
      <li>Q. Does it affect how experiments run jobs? A. Not much -
changes the “splitting” line a bit.</li>
      <li>Q. Don’t different generators do this already? A. In some
areas. Would want this to be completely open, feed back to
theorists.</li>
      <li>Want to share more widely, but LHCb are in a different region
of phase space, not clear real savings would happen.
        <ul>
          <li>Can also share intermediate pieces of computation.</li>
        </ul>
      </li>
      <li>Q. Who would actually do this? How to get credit for it. A.
Give DOIs to the data (CERN product - gives attribution).
Figure out how to actually fund people.</li>
      <li>Q. What about <strong>cross-checks</strong>? Experiments do this today
because they are independent. Past experience shows that these
checks are important. A. Yes, cross-checks are important. SH -
can do this with different pieces of samples. Q. But what
about different physics? A. Would have these differently for
each generator anyway, so that allows for checking across
different generators. This also alleviates the technical
difficulties. Josh - we don’t cross check at all at the
generator level today, so we’re not really losing much.</li>
      <li>Q. Generating event weights is fast for LO, not much saved
(detector simulation is the expensive bit). Different for
N(N)LO though. A. Parton level calculation only - can, e.g.,
combine samples.</li>
      <li>Q. Intermediate suggestion - common configuration of cards.
Share these as they are difficult to get right. A. Marek - for
Sherpa this is no-brainer. But traditionally ATLAS and CMS
could not agree on a common tuning for Pythia.</li>
      <li>Josh - scrutiny should actually go up! We would have two sets
of eyes on the setup. Simone - without this we may be
<em>limited</em> in N(N)LO computations and would lose physics. Frank
        <ul>
          <li>having more theory input helps here too. Peter - beware of
diminished responsibility.</li>
        </ul>
      </li>
      <li>Q. Andrea - does this help with correlation of systematics?
Yes. But the shower tunings also important.</li>
      <li>Economics of doing this? People are not being “paid” to do
this - just part of their physics analysis. What’s the
motivation to help the other experiment? Don’t want to lose
the analyst support. Josh - not sure this distinction is true.
MC convenors are analysts. Davide - everyone needs the common
samples (W+jets). Common samples will be managed “centrally”.
Zach - V+Jets, Single Top, and done - not so many samples.</li>
      <li>Simone - sociologically it would seem to work better to do a
common sample rather than share existing things. Get buy-in
from people who will use this.</li>
    </ul>
  </li>
  <li>3 - Other topics
    <ul>
      <li>Technical RAs making “deep” contributions into MC teams.
        <ul>
          <li>Zach - experiments are not dripping with software experts.
What person would do this job? Need a career path.</li>
          <li>Liz - another way to help would be to modernise software
practice, open up code to allow pull requests. A. Code has
always been open. Current version of HEPForge should
support direct pull requests. Andy - communication is
important, needs to be a conversation between main
developers and submitter. Pull requests out of the blue
are not desirable, given complexity of the systems and
potential side-effects.</li>
          <li>Servesh - need a dialog between engineers and physicists.
            <ul>
              <li>Andy (on Vidyo): “Since time &amp; question bandwidth is
limited, I may as well write here that I don’t think
pull-request GUIs are a silver bullet for getting
experiment RA/sw engineering contributions to MC
tools! They’re nice, but a minor effect IMHO.</li>
              <li>The sw/performance issues are *big*, and require
significant time investments. If this work isn’t
valued and rewarded, most will not bother… the
social &amp; career incentive obstacles are much bigger
than the mechanism for providing patches.</li>
              <li>Plus, understanding these codes in sufficient detail
to make significant changes is not something that
comes without a detailed &amp; lengthy conversation with
the authors.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Better alignments to needs of experiments
        <ul>
          <li>Ease of use is a big point to improve on.</li>
          <li>Andy - ATLAS are generating a lot of samples with heavy
filters - not very efficient. Generator side improvements
(biasing) would help a lot.</li>
          <li>Marek - had a student to work on common API, but problem
is that it’s only applicable to simplest processes.</li>
          <li>Josh B - analysis groups will do as little work as
possible (even if it’s wasteful). Need to push groups to
do this and also teach people how to use the generator
functionality properly. Zach - examples of issues from
ATLAS could be given back to theory people and used to
improve the documentation. Marek - need the documentation
there, but people also need to read it (!) to avoid
inefficient setups.</li>
          <li>Zach - would be great to have a “production mode” switch
for generators (like Madgraph), turn off all the bells and
whistles.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="photos-and-tauola-status-and-plans">Photos and Tauola Status and Plans</h4>
<ul>
  <li>Q. Is there any plan for being thread-safe? A. Initialisation is
done in one place - common block. Can pass that to different
“workers”. Not too worried about speed, it’s fast.</li>
  <li>Liz - experiments can deal with pattern of initialisation serial,
but event loop needs to share data read-only. A. There is one
place where data would be shared (histogram); different random
number generators can be plugged in already.</li>
</ul>

<h4 id="evtgen-status-and-plans">EvtGen Status and Plans</h4>
<ul>
  <li>EvtGen written and maintained by experimentalists.</li>
  <li>Modernisation from Gerhard has now overwhelmed GitLab web
interface (!) - many improvements!</li>
  <li>Multi-threading is next, will require new interface to the random
number generator</li>
  <li>HepMC 3 transition will take a few more months and help with
threading issues.</li>
  <li>Pythia8 could be used to replace Tauola; but Photos functionality
is really needed, no current alternative.</li>
  <li>Profiling shows lots of low hanging fruit for investigation</li>
  <li>Dealing with HepMC files (used in Photos interaction) takes a lot
of time.</li>
  <li>Q. Z - which version of photos and tauola are used? A. Think it is
the latest one.
    <ul>
      <li>Changing interface to “shared” object would be very hostile
for threading.</li>
    </ul>
  </li>
  <li>Two postdocs will contribute to multi-threading conversion effort,
as well as some help from Michel.</li>
  <li>Q. Zach - How do you get the decay table?
    <ul>
      <li>PDG tables cannot be used directly in a computer problem
(~disputed).
        <ul>
          <li>Problem with branching fractions.</li>
        </ul>
      </li>
      <li>Is this something HSF can help with?</li>
    </ul>
  </li>
</ul>

<h2 id="tuesday-pm">Tuesday PM</h2>

<h4 id="nnlo--n3lo-calculations-with-nnlojet">NNLO / N^3LO calculations with NNLOJet</h4>
<ul>
  <li>NNLOJet is framework for NNLO calculations using Antenna
subtraction - fortran + openmp + python, depends on LHAPDF.</li>
  <li>Three processing phases, but dominated by random-number seed
driven “production” phase -</li>
  <li>Jumps to N3LO from NNLO increases CPU by 2-3 orders of magnitude.</li>
  <li>How does this get then used by experiments (eg, to put 100k hours
in perspective) - experiments answer that they don’t this in
production - may be via analyst driven work. Sounds like this
might evolve towards HL-LHC - eg, precision needed in MC event
generation. [reweighting + final theory comparisons as opposed to
MC events]
    <ul>
      <li>Q/Andrea: This is not unweighted. How will experiments use
this? My mental model now is we will do LO unweighted (ok),
some NLO unweighted (problem with -ve weights, but okish),
plus complementary info from NNLO, eg some distributions with
very fine binning for some specific analyses. Is this a
correct model? A/Simone: yes now this is what we can do, but
in 10 years we will probably have this NNLO unweighted and
this will be the bottleneck. C/Andrea: yes but the question is
whether the NNLO unweighted will be money affordable at that
time…</li>
    </ul>
  </li>
  <li>Scales “badly” with # of particles. Something to follow</li>
  <li>HPC discussion suggests that this sort of calculation is a good
use case.
    <ul>
      <li>CERN’s collaboration with PRACE may be useful here</li>
    </ul>
  </li>
</ul>

<h4 id="nnlo-calculations-with-matrix">NNLO calculations with Matrix</h4>
<ul>
  <li>Matrix requirement sare LHAPDF and numpy (plus more dependencies
inside)</li>
  <li>Parallelizm is across processes, batch support built in</li>
  <li>Nice comparison of CPU vs uncertainty - bottom line - days @ NNLO
vs minutes @ NLO vs seconds @ LO</li>
</ul>

<h4 id="nnlo-calculations-with-mcfm">NNLO calculations with MCFM</h4>
<ul>
  <li>Linear scaling via MPI - 1% precision on NNLO cross section O(1
hour) on desktop. CPU for processes with jet O(1000 hours)</li>
</ul>

<h4 id="beyond-current-paradigms">Beyond Current Paradigms</h4>
<ul>
  <li>Fixed order calculations - cross sections, not really events
anymore. Or are we just doing “more of the same”?</li>
  <li>Large production runs will probably become inaccessible to
theorists.</li>
  <li>Andrea - will probably always need unweighted events with detector
simulation. Driven by physics.</li>
  <li>Lattice get specific grants to do specific calculations -
something like this probably becomes the norm. Would be infrequent
and require some community agreement.</li>
  <li>Accuracy of the calculations - if something takes 6 months to run
then this is probably not debuggable - then not trustable. If
parallalisable then it could easily foresee 10k cores for some
hours.</li>
  <li>HSF/LPCC group proposal - identify interested people to build on
the sort of discussions during this workshop.
    <ul>
      <li>Andrea volunteered with computing resources point of view</li>
      <li>
    </ul>
  </li>
  <li>Unfolding - experiments have very different needs in this area.
Not clear if increasing order helps - often harder to match to
data. Recent ATLAS paper (Hbb?) just used Pythia8, but with
sophisticated unfolding.
    <ul>
      <li>Key point is to describe the data well - if LO does that then
could be enough.</li>
      <li>Should be testing unfolding in a number of different
scenarios.</li>
    </ul>
  </li>
  <li>Should be careful about what we mean by “right” - uncertainties
always exist and we have to balance them. Recent changes towards
investment in understanding theory systematics</li>
  <li>Inclusive cross sections and unweighted events have both proved to
be useful in LHC analysis.</li>
  <li>Landscape changing - era of ‘fast’ searches are maybe coming to an
end.</li>
  <li>Back to negative weights - can we judge if we are happy with 20%
negative weights? Introduces the discussion of reweighting schemes
and their use for systematics. When are they robust enough to
avoid sending events through G4 simulation.</li>
  <li>Lots of theory advances - this workshop should focus on areas
where improving the codes’ engineering will help.
    <ul>
      <li>Need people to work together on this problem.</li>
      <li>Testing important.</li>
    </ul>
  </li>
  <li>If there’s a need there should be a reward!</li>
  <li>Codes are evolving all the time - not clear that the languages
used are optimal, but that’s the code we have. Be simple.</li>
</ul>

<p>Take Home Messages / Actions / Planning</p>
<ul>
  <li>Generator performance comparisons work
    <ul>
      <li>Broaden this activity</li>
      <li>Understand differences between ATLAS and CMS - e.g. two things
to understand
        <ul>
          <li>Comparison of sherpa and MG for the same process</li>
          <li>Are the two experiments producing very different types of
samples or configurations? Is ATLAS producing more or more
expensive events?</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>LHCb is different from ATLAS/CMS as generation focuses on decays</li>
  <li>Regression tests (for physics results AND timing performance) and
best code practice
    <ul>
      <li>The HSF can help with this (a lot of technical experts)</li>
      <li>Can the experiments help with their infrastructure for timing
regression?</li>
    </ul>
  </li>
  <li>Projection of CPU needs for Run4
    <ul>
      <li>The fractional budget of event generation will increase as
experiments move to fast simulation and faster reconstruction</li>
      <li>Model for use of LO, NLO and NNLO?</li>
    </ul>
  </li>
  <li>Best ways to support technical work in generator field
    <ul>
      <li>Experiments</li>
      <li>Engineers (how to fund this - grant applications?)</li>
      <li>Prospects for ‘permanent’ jobs (i.e. not just two year grants)</li>
    </ul>
  </li>
  <li>Generators critical part of what experiments do. Aware that
“reward system” has limitations, need to improve on this.
    <ul>
      <li>Physics improvements are what gets noticed in the theory
community and therefore gets funding and jobs. Example
speed/memory improvement in Herwig (matrix elements and NLO
integration) would be “many years of work for little
recognition”. (see Peter’s presentation)</li>
    </ul>
  </li>
  <li>GPU ports / code portability? Are experiments interested in
using/validating, particularly Madgraph.
    <ul>
      <li>Supercomputers/HPCs are going more and more towards GPUs.</li>
    </ul>
  </li>
  <li>A dedicated performance workshop in the future (Spring?)
    <ul>
      <li>Say 5 days of real hands-on work</li>
    </ul>
  </li>
  <li>Negative weights are a problem (but only in NLO, there are none in
LO). These come from cancellations of real and virtual
contributions and are unavoidable. “Unweighted” events with
negative weights means that events have weight +1 or -1
essentially.
    <ul>
      <li>Need better metrics to understand the impact of negative
weights. Example: scalar metrics integrating the knowledge of
the weight distribution and also the relative cost (in CPU
budget) of detector simulation vs generation.</li>
    </ul>
  </li>
  <li>Can a common hook be added to generators so that we can set
generator level cuts in a standard way?</li>
  <li>HSF has Working Groups in important areas of work for HEP. This
area could benefit from the same.
    <ul>
      <li>Some convenors willing to carry this forward and organise?</li>
      <li>Collaboration with LPCC has been very fruitful - should be
joint activity</li>
    </ul>
  </li>
</ul>


<hr>

<div class="PageNavigation">

  <!-- Note that page sorting is reverse time ordered, so "next" refers to the previous
       meeting in time and "previous" to the next one -->
  
   <p class="alignleft"><a href="/organization/2018/11/21/softwareforum.html"> ← HSF Software Forum on EIC Software Consortium, November 21, 2018</a></p>
  

  
    <p class="alignright"><a href="/organization/2018/11/29/coordination.html">HSF Weekly Meeting #152, 29 November, 2018 → </a></p>
  

</div>

<div style="clear: both;"></div>


<br><br>
<div class="footer fixed-bottom">
Thanks to <a href="https://pages.github.com/">GitHub Pages</a>, <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://getbootstrap.com/">Bootstrap</a>
</div>

</div> <!-- container -->


<!-- Google Analytics -->

<!-- Google Analytics end -->

<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
</body>
</html>
